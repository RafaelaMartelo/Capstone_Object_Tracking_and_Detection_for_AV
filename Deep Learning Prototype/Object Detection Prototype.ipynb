{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efee369e",
   "metadata": {},
   "source": [
    "# Prototype: Object Detection for Autonomous Vehicles in Inclement Weather\n",
    "\n",
    "<img src=\"images/test_5.jpg\" alt=\"Description or alt text for the image\" title=\"Optional title\" width=\"600\"/>\n",
    "\n",
    "Figure 1: Example of Object Detection with Inclement Weather\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Dataset and Augmentation Strategy\n",
    "To advance our object detection prototype, we've curated a specialized dataset, subsequently enriched through strategic augmentation to bolster model training and performance. Leveraging **Roboflow** for its robust image augmentation capabilities, we enhanced our dataset, initially comprising 198 images, by introducing realistic variations like blur (up to 1.7px) and noise (affecting up to 0.3% of pixels). This approach aimed to simulate the challenges inherent in inclement weather conditions, directly impacting autonomous vehicle navigation.\n",
    "\n",
    "**Augmented Dataset Composition:**\n",
    "- **Training Set**: Enriched to 278 images for comprehensive learning.\n",
    "- **Validation Set**: 39 images for parameter optimization.\n",
    "- **Test Set**: 20 images for evaluating model efficacy.\n",
    "\n",
    "**Label Distribution:**\n",
    "- **Cars**: Predominantly featured with 1,178 labels.\n",
    "- **Trucks**: Represented by 134 labels.\n",
    "- **Persons**: Included with 278 labels to mimic real-world scenarios.\n",
    "\n",
    "### 1.2 Fine-tuning Strategy\n",
    "\n",
    "#### Batch Size\n",
    "- **Evaluation**: Tested with batch sizes 8 and 16, acknowledging the default of 16 as optimal.\n",
    "- **Justification**: Balances computational demands with learning efficiency, crucial for training on augmented datasets without compromising on model training effectiveness.\n",
    "\n",
    "#### Learning Rate Scheduler\n",
    "- **Adoption**: Utilized a cosine learning rate scheduler, enhancing the default setting.\n",
    "- **Advantage**: Facilitates better model performance through adaptive learning rate adjustments, avoiding local minima and refining model weights more precisely over time.\n",
    "\n",
    "#### Dataset Expansion\n",
    "- **Implementation**: Nearly doubled the dataset size, integrating data augmentation to introduce a wider array of visual scenarios.\n",
    "- **Weather Conditions**: Dataset inlcudes cloudy and snowy weather conditions.\n",
    "- **Objective**: To enhance the model's robustness and its ability to generalize, ensuring reliable performance under diverse environmental conditions, including inclement weather.\n",
    "\n",
    "## 2. Experiment Overview and Configurations for Inclement Weather Object Detection\n",
    "\n",
    "To advance the capabilities of our object detection model specifically designed for inclement weather conditions, we conducted a series of experiments focusing on the YOLOv8 trained model. The objective was to fine-tune the model's settings—particularly the learning rate scheduler and batch size—to identify the optimal configuration that ensures both high accuracy and computational efficiency.\n",
    "\n",
    "### 2.1. Experiment Configurations\n",
    "\n",
    "### Learning Rate Scheduler\n",
    "- **Description**: Adjusts whether the cosine learning rate scheduler is active, dynamically changing the learning rate in a cosine pattern throughout training.\n",
    "- **Options**: Enabled (`True`) or Disabled (`False`).\n",
    "- **Goal**: Investigate the scheduler's role in enhancing the model's ability to generalize and avoid local minima, crucial for robust detection in varying weather conditions.\n",
    "\n",
    "### Batch Size\n",
    "- **Description**: Specifies the number of training examples the model processes in a single iteration.\n",
    "- **Options**: `8` and `16`.\n",
    "- **Goal**: Assess the impact of batch size on the learning process and computational load, aiming to find a balance that optimizes both learning efficiency and resource usage.\n",
    "\n",
    "### Conducted Experiments\n",
    "\n",
    "Four experiments were designed with different configurations to explore how each affects the performance of the model in detecting objects under inclement weather:\n",
    "\n",
    "1. **Experiment 1 - 8T**: Batch Size of 8, Learning Rate Scheduler Enabled.\n",
    "2. **Experiment 2 - 8F**: Batch Size of 8, Learning Rate Scheduler Disabled.\n",
    "3. **Experiment 3 - 16T**: Batch Size of 16, Learning Rate Scheduler Enabled.\n",
    "4. **Experiment 4 - 16F**: Batch Size of 16, Learning Rate Scheduler Disabled.\n",
    "\n",
    "The aim was to isolate the effects of these two parameters on the model's accuracy and processing efficiency, providing insights into the most effective training regimen for inclement weather object detection.\n",
    "\n",
    "### 2.2. Objective\n",
    "\n",
    "The primary goal of these experiments is to fine-tune the YOLOv8 model for enhanced performance in inclement weather scenarios. By optimizing training parameters, we strive to improve the model's precision in detecting objects amidst challenging weather conditions, an essential capability for various applications, including autonomous vehicle navigation and outdoor surveillance.\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "The detailed training and validation metrics for each experiment will be discussed later in this notebook. This section is dedicated to summarizing and interpreting the outcomes, focusing on their implications for developing a more accurate and efficient object detection system for inclement weather. The comprehensive analysis aims to facilitate a straightforward understanding of the experimental findings and their relevance to the project's objectives.\n",
    "\n",
    "\n",
    "## 3. Results and Analysis\n",
    "\n",
    "The fine-tuning of our YOLOv8 Small model, with a focus on batch size and learning rate adjustments, led to significant improvements in performance metrics. This section delves into the detailed outcomes of our experiments, particularly highlighting the performance of the model configuration labeled as \"16T\" - which denotes a batch size of 16 and an enabled cosine learning rate scheduler.\n",
    "\n",
    "<img src=\"images/confusion_matrix_normalized.png\" alt=\"Description or alt text for the image\" title=\"Optional title\" width=\"700\"/>\n",
    "Figure 2: Confusion Matrix Normalized.\n",
    "\n",
    "### 3.1.Analysis of Normalized Confusion Matrix Results\n",
    "\n",
    "The normalized confusion matrix provides a detailed view of the model's classification performance, with a focus on the proportion of correct and incorrect predictions for each class, normalized by the number of true instances for those classes. This perspective is particularly valuable for understanding the model's behavior in a balanced manner, especially when dealing with imbalanced datasets.\n",
    "\n",
    "### Key Observations from the Normalized Confusion Matrix\n",
    "\n",
    "**Analysis of the Confusion Matrix Results**\n",
    "\n",
    "The confusion matrix offers invaluable insights into the performance of our object detection model, particularly regarding its ability to accurately classify different objects within flood scenarios. Here's a detailed analysis based on the diagonal and off-diagonal values observed in the matrix:\n",
    "\n",
    "**High Diagonal Values**\n",
    "- The diagonal values of the confusion matrix represent the model's prediction accuracy for each class. Notably, the **truck** class achieves the highest accuracy with a value of **0.86**, indicating exceptional model performance in recognizing trucks. This is followed by the **person** class with an accuracy of **0.83**, and the **car** class with an accuracy of **0.8**. These findings suggest a hierarchy in detection capabilities, with trucks being identified most reliably.\n",
    "\n",
    "**Off-diagonal Values and Misclassification Trends**\n",
    "- Off-diagonal values reveal instances where the model confuses one class for another. A significant observation is that the **car** class is frequently misclassified as **background**, leading to missed detections. This misclassification significantly affects the car's lower accuracy despite being the predominant label in our dataset. Furthermore, the **person** class is often misclassified as **background** as well.\n",
    "\n",
    "<img src=\"images/results.png\" alt=\"Description or alt text for the image\" title=\"Optional title\" width=\"700\"/>\n",
    "\n",
    "Figure 3: mAP and loss plots after training the YOLOv8 Small model on the dataset.\n",
    "\n",
    "<img src=\"images/train_batch2.jpg\" alt=\"Description or alt text for the image\" title=\"Optional title\" width=\"700\"/>\n",
    "\n",
    "Figure 4: Example training batch 2 of the dataset.\n",
    "\n",
    "### 3.2. Detailed Performance Metrics for 16T Configuration\n",
    "\n",
    "The 16T configuration yielded the following enhanced performance metrics, showcasing notable improvements across various aspects:\n",
    "\n",
    "- **Precision (Box(P))**: Achieved a high precision rate of 86.2%, indicating the model's accuracy in predicting relevant objects across all classes. This metric is particularly crucial for autonomous vehicles, where false positives can lead to unnecessary or hazardous maneuvers.\n",
    "  \n",
    "- **Recall (R)**: Recorded at 77.5%, this metric reflects the model's ability to correctly identify all relevant instances in the dataset. Although there's a slight decrease from the pre-fine-tuning state, the trade-off is balanced by substantial gains in precision.\n",
    "\n",
    "- **mAP0.5**: With an mAP of 84.3%, the model demonstrates strong consistency in detecting objects accurately at the 0.5 IoU threshold. This improvement is indicative of the model's robustness in various detection scenarios.\n",
    "\n",
    "- **mAP0.5:0.95**: The increase to 58.9% in this metric highlights the model's improved performance across a range of IoU thresholds, showcasing its versatility and reliability in more precise object detection tasks.\n",
    "\n",
    "### Class-Specific Performance Insights\n",
    "\n",
    "- **Cars**: The model maintained high performance in car detection, a critical aspect for navigating traffic and urban environments, with a precision of 85.8% and an mAP0.5 of 86.1%.\n",
    "  \n",
    "- **Trucks**: Notably, truck detection saw a remarkable improvement, with precision jumping to 94.7% and mAP0.5 reaching 86.9%. This enhancement underscores the model's increased capability in identifying larger vehicles, vital for ensuring safety on highways and in mixed-traffic situations.\n",
    "  \n",
    "- **Persons**: The introduction of person detection with a precision of 78.1% and an mAP0.5 of 80.0% marks a significant step forward. Despite being a newly added class, the model's ability to detect pedestrians enhances its application in ensuring pedestrian safety around autonomous vehicles.\n",
    "\n",
    "\n",
    "### Key Highlights\n",
    "- **Precision** improved overall from 75.9% to 86.2%, indicating higher accuracy in the model's predictions.\n",
    "- **Recall** experienced a minor reduction from 79.2% to 77.5%, a slight trade-off for the gain in precision.\n",
    "- **Mean Average Precision (mAP50)** increased from 80.6% to 84.3%, with **mAP50-95** also rising from 54.2% to 58.9%, demonstrating enhanced detection reliability across varying Intersection over Union (IoU) thresholds.\n",
    "\n",
    "\n",
    "### 3.3. Reflection on Dataset Imbalance and Model Performance\n",
    "\n",
    "The imbalanced nature of the dataset, with a predominant focus on cars, necessitates careful consideration. While the model demonstrates great ability in detecting vehicles, the relatively lower representation of trucks and persons could influence its learning bias:\n",
    "\n",
    "- **Oversampling Underrepresented Classes**: By artificially increasing the number of instances of underrepresented classes in the training dataset, we can ensure that the model receives adequate exposure to these classes, reducing the risk of overlooking them.\n",
    "\n",
    "- **Advanced Data Augmentation Techniques**: Employing a variety of data augmentation techniques such as random rotations, scaling, cropping, and color adjustments can introduce more variability into the training data. This not only helps in addressing class imbalance but also in improving the model's robustness to diverse conditions.\n",
    "\n",
    "- **Adjusting Class Weights**: During the training phase, assigning higher weights to underrepresented classes can compel the model to pay more attention to minimizing errors on these classes. This adjustment helps in balancing the influence of each class on the model's learning process.\n",
    "\n",
    "- **Expanding the Dataset with More Diverse Scenarios**: In addition to the above strategies, expanding the dataset to include a broader range of scenarios for all three classes—cars, trucks, and persons—can introduce more diversity into the model's training regime. Incorporating images from different weather conditions, times of day ensures that the model learns to recognize these classes across a wide spectrum of real-world situations. This expansion not only addresses class imbalance but also enhances the model's generalization capabilities, making it more effective in accurately detecting objects in complex driving scenarios.\n",
    "\n",
    "By combining these strategies, we can significantly improve the model's ability to detect and classify objects with greater accuracy and fairness across all classes. This holistic approach to training will pave the way for developing a highly reliable object detection system for autonomous vehicles.\n",
    "\n",
    "## Conclusion\n",
    "The fine-tuned YOLOv8 Small model, supported by an augmented and expanded dataset, showcases promising advancements in object detection crucial for autonomous vehicles in inclement weather. This project exemplifies the importance of targeted dataset preparation, strategic parameter adjustment, and the ongoing need to address data imbalances for optimal model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378f252",
   "metadata": {},
   "source": [
    "## 4. Experiment Details\n",
    "\n",
    "### 4.1. Experiment Configurations\n",
    "\n",
    "- **Learning Rate Scheduler**: Indicates whether the cosine learning rate scheduler was enabled (`True`) or disabled (`False`). The scheduler adjusts the learning rate in a cosine pattern, potentially aiding in avoiding local minima and achieving better generalization.\n",
    "- **Batch Size**: The number of training examples utilized in one iteration. We explored sizes of `8` and `16` to understand their impact on the model's learning dynamics and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513ebcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.0.225, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in object_detect_proto-2 to yolov8:: 100%|██████████| 12511/12511 [00:02<00:00, 4227.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to object_detect_proto-2 in yolov8:: 100%|██████████| 686/686 [00:00<00:00, 826.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"IvxuC5AtkVBPx3o8pcGV\")\n",
    "project = rf.workspace(\"machine-learning-engineer-bootcamp\").project(\"object_detect_proto\")\n",
    "dataset = project.version(2).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b88696af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Setup complete ✅ (4 CPUs, 15.8 GB RAM, 239.6/953.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94232f",
   "metadata": {},
   "source": [
    "### Experiment 1 - 8T (Batch Size: 8, Learning Rate Scheduler: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8f7f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.11 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=50, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train8\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train8', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\valid\\labels.cache\n",
      "Plotting labels to runs\\detect\\train8\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train8\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.272      0.412       0.23       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.273      0.388      0.307      0.131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.395      0.493      0.364      0.149\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.347      0.489      0.404      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.256      0.341      0.283      0.136\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.381      0.483      0.415      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.578      0.527      0.564       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.578      0.568      0.599      0.297\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.627       0.55      0.618      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.599      0.544      0.596      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.662      0.552      0.618      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.641      0.572      0.606      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.797      0.544      0.641      0.355\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.724      0.592      0.666      0.396\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.808      0.522      0.654      0.384\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.79      0.532      0.699      0.422\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.698       0.64      0.726      0.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.822      0.609      0.718      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.812      0.617      0.717      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.837      0.585      0.722      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.847      0.609      0.742      0.455\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.856      0.591      0.753      0.476\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.78      0.679       0.76      0.462\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.738      0.678      0.761      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.803      0.688      0.773       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.825      0.689      0.771      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.856      0.647      0.771      0.492\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.811      0.673      0.773       0.48\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.797      0.709      0.774       0.48\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.852      0.673      0.777        0.5\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.764      0.684       0.77      0.482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.785      0.685      0.783      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.837      0.694      0.794      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.889      0.663      0.804      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.832      0.711      0.802      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.889       0.68        0.8       0.53\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.879      0.671      0.798      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.883      0.678      0.806       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.852      0.711      0.808      0.552\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290        0.8      0.717      0.794      0.544\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.806      0.693      0.793      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.844      0.708      0.807      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.88      0.694      0.819      0.546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.888      0.688      0.818      0.552\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.891      0.688      0.816      0.546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.881       0.69      0.816      0.546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.882      0.698      0.818      0.555\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.883      0.694      0.816      0.558\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.877      0.695      0.815      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.872      0.695      0.815      0.559\n",
      "\n",
      "50 epochs completed in 9.235 hours.\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train8\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                   all         39        290      0.872      0.695      0.815      0.559\n",
      "                   car         39        221      0.906      0.692      0.851      0.598\n",
      "                person         39         48      0.804      0.583      0.713      0.382\n",
      "                 truck         39         21      0.905       0.81      0.882      0.697\n",
      "Speed: 7.4ms preprocess, 860.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "# Code that produces output\n",
    "\n",
    "model_1 = YOLO('yolov8s.pt')  # load a pretrained model \n",
    "# Train the model\n",
    "results = model_1.train(data='data.yaml', epochs=50, imgsz=640, batch= 8, cos_lr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c173a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\valid\\labels.cache... 39 images, 6 backgrounds, 0 corrupt: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:31<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         39        290      0.872      0.695      0.815      0.559\n",
      "                   car         39        221      0.906      0.692      0.851      0.598\n",
      "                person         39         48      0.804      0.583      0.713      0.382\n",
      "                 truck         39         21      0.905       0.81      0.882      0.697\n",
      "Speed: 6.6ms preprocess, 779.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train82\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.59774,     0.38205,      0.6966])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_1.val()\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af568a24",
   "metadata": {},
   "source": [
    "### Experiment 2 - 8F (Batch Size: 8, Learning Rate Scheduler: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e8e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.11 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=50, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train9\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train9', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Plotting labels to runs\\detect\\train9\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train9\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.272      0.412       0.23       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.359      0.292      0.293      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.597      0.348      0.344      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.677      0.406      0.462      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.433      0.476      0.485      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.423      0.593      0.492      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.597      0.444      0.495      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.67      0.597      0.633      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.707      0.576      0.653      0.349\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.634        0.5      0.588      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.728      0.555      0.631      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.702      0.514      0.658      0.346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.804      0.555      0.691      0.387\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.72      0.508      0.649      0.384\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.758      0.649      0.724      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.784      0.633      0.722      0.417\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.766      0.611      0.718      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.691      0.624      0.695      0.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.772      0.597      0.718       0.43\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.821      0.609      0.732      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.857      0.615      0.754      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.762       0.68      0.748      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.816      0.654      0.778      0.485\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.751      0.643      0.752      0.467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.774       0.66      0.762      0.467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.803      0.651      0.762      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.836      0.658      0.775      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.81      0.691      0.786      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.885      0.638      0.791      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.859       0.66      0.797       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.844      0.668       0.79      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.833      0.691      0.805      0.526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.783      0.752      0.806      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.778      0.738       0.81      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.822      0.712      0.798      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.783      0.752      0.801      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.778      0.723      0.802      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.851      0.617      0.785      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.82      0.737      0.792       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.858      0.677      0.793      0.523\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.872      0.666      0.795      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.863      0.697      0.811      0.535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.885      0.687      0.809      0.542\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.868      0.678        0.8      0.532\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.777      0.755      0.818      0.548\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.849      0.695      0.817      0.551\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.825      0.708      0.806       0.56\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.826      0.704      0.801      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.812      0.707      0.804      0.558\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.824      0.705       0.81      0.565\n",
      "\n",
      "50 epochs completed in 9.403 hours.\n",
      "Optimizer stripped from runs\\detect\\train9\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train9\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train9\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                   all         39        290      0.824      0.705       0.81      0.565\n",
      "                   car         39        221      0.881      0.702      0.861      0.616\n",
      "                person         39         48      0.751      0.604      0.708      0.398\n",
      "                 truck         39         21      0.841       0.81      0.862       0.68\n",
      "Speed: 8.1ms preprocess, 1045.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "# Code that produces output\n",
    "\n",
    "model_2 = YOLO('yolov8s.pt')  # load a pretrained model \n",
    "# Train the model\n",
    "results = model_2.train(data='data.yaml', epochs=50, imgsz=640, batch= 8, cos_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c5d342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\valid\\labels.cache... 39 images, 6 backgrounds, 0 corrupt: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:38<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         39        290      0.824      0.705       0.81      0.565\n",
      "                   car         39        221      0.881      0.702      0.861      0.616\n",
      "                person         39         48      0.751      0.604      0.708      0.398\n",
      "                 truck         39         21      0.841       0.81      0.862       0.68\n",
      "Speed: 7.6ms preprocess, 936.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train92\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     0.6163,     0.39814,      0.6801])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_2.val()\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda46d98",
   "metadata": {},
   "source": [
    "### Experiment 3 - 16T (Batch Size: 16, Learning Rate Scheduler: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e60f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.11 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train10\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train10', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Plotting labels to runs\\detect\\train10\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train10\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.37      0.296      0.265      0.125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.668      0.303      0.387      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.561      0.402      0.442      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.533      0.284      0.336      0.176\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.474      0.385      0.391      0.192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.653      0.365      0.462      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.542      0.472      0.454      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.652       0.53      0.602      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.657      0.563       0.63      0.321\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.797      0.537       0.63      0.359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.729      0.542      0.626      0.358\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.721      0.576      0.647      0.383\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.762       0.57      0.661      0.372\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.686      0.519      0.615      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.742      0.593      0.718      0.411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.877      0.587      0.683      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.856      0.606       0.71       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.889      0.594      0.751      0.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.836      0.634      0.746      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.839      0.598      0.746      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.816      0.633       0.75      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.787      0.672      0.756       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.842      0.648      0.779      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.846      0.672      0.791      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.782       0.67      0.761      0.471\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.827      0.682      0.768      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.792      0.677      0.779       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.798      0.742      0.794      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.801       0.68      0.797      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.867      0.691      0.802      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.834      0.686      0.808      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.828      0.726      0.807      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.876      0.688      0.803      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.796      0.714      0.816      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.849      0.718      0.833      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.88      0.699      0.832      0.567\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.844      0.751      0.836      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.843      0.749      0.833      0.556\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.864      0.745      0.842      0.548\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.846      0.744      0.832      0.553\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.854      0.725      0.827      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.853      0.763       0.84      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.863       0.77      0.844      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.879      0.764      0.838      0.577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.864       0.76      0.836      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.848      0.771      0.835       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.855      0.777      0.842      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.869      0.771      0.843      0.589\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.825      0.788      0.842      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.833      0.784      0.849      0.588\n",
      "\n",
      "50 epochs completed in 9.904 hours.\n",
      "Optimizer stripped from runs\\detect\\train10\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train10\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train10\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                   all         39        290      0.862      0.775      0.843      0.589\n",
      "                   car         39        221      0.858      0.765      0.861      0.608\n",
      "                person         39         48      0.781      0.708        0.8      0.457\n",
      "                 truck         39         21      0.947      0.852      0.869      0.702\n",
      "Speed: 8.6ms preprocess, 812.4ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "# Code that produces output\n",
    "\n",
    "model_3 = YOLO('yolov8s.pt')  # load a pretrained model \n",
    "# Train the model\n",
    "results = model_3.train(data='data.yaml', epochs=50, imgsz=640, batch= 16, cos_lr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bf0810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\valid\\labels.cache... 39 images, 6 backgrounds, 0 corrupt: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:32<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         39        290      0.862      0.775      0.843      0.589\n",
      "                   car         39        221      0.858      0.765      0.861      0.608\n",
      "                person         39         48      0.781      0.708        0.8      0.457\n",
      "                 truck         39         21      0.947      0.852      0.869      0.702\n",
      "Speed: 6.5ms preprocess, 792.8ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train102\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.60849,     0.45694,      0.7024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_3.val()\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc380744",
   "metadata": {},
   "source": [
    "### Experiment 4 - 16F (Batch Size: 16, Learning Rate Scheduler: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05f65e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.11 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train11\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train11', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Plotting labels to runs\\detect\\train11\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train11\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.37      0.296      0.265      0.125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.73      0.305      0.364      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.561      0.345      0.426      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.629       0.37      0.389       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.674      0.369       0.47      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.556      0.398      0.423      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.511      0.405      0.386      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.627       0.28      0.388      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.507      0.509      0.432      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.765      0.514      0.617      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290        0.7      0.553       0.64      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.865      0.477       0.63      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.761      0.561      0.677      0.376\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.839      0.547      0.697       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.889      0.523      0.705      0.404\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.843      0.584      0.711      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.834      0.612      0.744      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.764      0.683      0.757      0.439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.813      0.662      0.744      0.443\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.736       0.69      0.745       0.43\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.767      0.673      0.768      0.464\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.884      0.694      0.782      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.84      0.699      0.804      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.88      0.664      0.772      0.501\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.849      0.708      0.784      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.82      0.653      0.769      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.883      0.696      0.805      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.855      0.683      0.791      0.526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.851      0.695       0.79      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.904      0.678      0.789      0.516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.805       0.72      0.809       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.927      0.627      0.786      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.853      0.654      0.793      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.808       0.68      0.779      0.535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.848      0.642      0.786      0.537\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.802      0.707      0.791      0.537\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.808      0.711      0.787      0.526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.873       0.71      0.802      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.835      0.699      0.802      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.818      0.709      0.814      0.562\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.716      0.731      0.791      0.543\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.801      0.754      0.808       0.55\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290       0.81      0.738      0.814      0.566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.846      0.707      0.813      0.567\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.841      0.714      0.819      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.822      0.724      0.828      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.847      0.731      0.832      0.572\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.865      0.731      0.835      0.575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.832      0.739      0.833      0.575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all         39        290      0.823      0.729      0.821      0.586\n",
      "\n",
      "50 epochs completed in 9.794 hours.\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train11\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                   all         39        290      0.822      0.729      0.822      0.586\n",
      "                   car         39        221      0.822      0.733      0.849      0.628\n",
      "                person         39         48      0.802      0.646      0.752      0.428\n",
      "                 truck         39         21      0.843       0.81      0.864      0.701\n",
      "Speed: 7.2ms preprocess, 827.3ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "# Code that produces output\n",
    "\n",
    "model_4 = YOLO('yolov8s.pt')  # load a pretrained model \n",
    "# Train the model\n",
    "results = model_4.train(data='data.yaml', epochs=50, imgsz=640, batch= 16, cos_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18d0b236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.225 🚀 Python-3.11.4 torch-2.1.1+cpu CPU (Intel Core(TM) i7-7600U 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rafae\\Documents\\Capstone\\object_detect_proto-2\\valid\\labels.cache... 39 images, 6 backgrounds, 0 corrupt: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:33<00:00, 11.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         39        290      0.822      0.729      0.822      0.586\n",
      "                   car         39        221      0.822      0.733      0.849      0.628\n",
      "                person         39         48      0.802      0.646      0.752      0.428\n",
      "                 truck         39         21      0.843       0.81      0.864      0.701\n",
      "Speed: 6.8ms preprocess, 823.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train112\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.62796,     0.42754,     0.70107])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_4.val()\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ee386",
   "metadata": {},
   "source": [
    "### 4.2. Comparative Analysis\n",
    "\n",
    "The experiments conducted to optimize our YOLOv8 model for inclement weather object detection provided valuable insights into the impact of learning rate scheduler activation and batch size variations. Below, we summarize the findings from each experiment and explain our decision to select the 16T configuration as the optimal setup.\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "- **Experiment 1 - 8T**:\n",
    "  - Duration: 9.235 hours\n",
    "  - Highest accuracy for truck detection (.882 mAP50), but overall performance for all classes (.815 mAP50) not optimal.\n",
    "  - Slightly faster inference time compared to other configurations.\n",
    "\n",
    "- **Experiment 2 - 8F**:\n",
    "  - Duration: 9.403 hours\n",
    "  - Moderate performance across all classes with a slight improvement in mAP50-95 scores compared to 8T.\n",
    "  - Longer inference time, indicating less efficiency.\n",
    "\n",
    "- **Experiment 3 - 16T** (Selected Configuration):\n",
    "  - Duration: 9.904 hours\n",
    "  - Showed the best balance between accuracy and efficiency, with the highest overall performance (.843 mAP50) and significant improvement in truck detection (.869 mAP50).\n",
    "  - Slightly longer training duration but compensated with improved performance and efficient inference time.\n",
    "\n",
    "- **Experiment 4 - 16F**:\n",
    "  - Duration: 9.794 hours\n",
    "  - Comparable training duration to 16T but with slightly reduced performance metrics across all classes.\n",
    "  - Inference speed and postprocess time did not justify the slight decrease in performance metrics.\n",
    "\n",
    "### Explanation for Selecting 16T Configuration \n",
    "\n",
    "The decision to proceed with the **16T configuration** as the prototype model for our project was based on a comprehensive evaluation of the trade-offs between training duration, accuracy across classes, and inference efficiency. Here's why 16T stood out:\n",
    "\n",
    "- **Optimal Performance**: The 16T configuration demonstrated superior ability to accurately detect objects across all tested classes, especially for trucks which are critical for inclement weather conditions. The improvement in precision and mAP scores for truck detection underlines the model's enhanced capability to differentiate between objects crucial for navigating adverse weather.\n",
    "\n",
    "- **Balanced Efficiency**: While the 16T experiment took slightly longer to complete training, the negligible increase in duration is justified by notable improvements in detection accuracy and model responsiveness. The balance between training time and performance metrics suggests a more robust model capable of delivering reliable results in real-world applications.\n",
    "\n",
    "- **Generalization and Inference Speed**: The enabled cosine learning rate scheduler in the 16T configuration likely contributed to better generalization, reducing the risk of overfitting and enhancing the model's adaptability to diverse inclement weather scenarios. Moreover, the efficient inference time ensures that the model can process images swiftly, an essential feature for real-time object detection applications.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Considering the critical importance of accuracy and efficiency in object detection for inclement weather, the 16T configuration emerges as the most suitable choice for our prototype. It strikes an optimal balance between high detection accuracy, particularly for critical classes like trucks, and practical operational efficiency. Future work will focus on further refining this model to enhance its reliability and performance in detecting objects under various inclement weather conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209bfe8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **YOLOv8 Predict Documentation**: [Ultralytics YOLOv8 Predict Documentation](https://docs.ultralytics.com/modes/predict/#inference-arguments) provides detailed insights on the inference arguments and usage of the YOLOv8 model for predictions.\n",
    "\n",
    "- **YOLO Resources**:\n",
    "  - For additional resources and configurations related to YOLO models, visit the Ultralytics GitHub repository: [Ultralytics Default Configuration](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50717892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
